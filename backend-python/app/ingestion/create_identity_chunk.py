from docling_core.types.doc import DoclingDocument
import os
import json
from typing import Dict, Any, Optional
from openai import AsyncOpenAI


async def create_identity_chunk(
    doc: DoclingDocument, 
    doc_id: str,
    doc_title: Optional[str] = None
) -> Dict[str, Any]:
    """
    Cr√©e un chunk identit√© condens√© pour un document.
    
    Args:
        doc: Document Docling partitionn√©
        doc_id: UUID du document
        doc_title: Titre du document (optionnel)
    
    Returns:
        dict avec:
        - identity_text: Le texte de la fiche identit√©
        - token_count: Nombre approximatif de tokens
        - pages_sampled: Pages utilis√©es pour l'analyse
    """
    print(f"üîÑ Cr√©ation de la fiche identit√© pour {doc_title or doc_id}...")
    
    # 1. Extraire le sommaire/table des mati√®res
    toc = extract_table_of_contents(doc) #table of contents
    
    # 2. √âchantillonner le document : 6 premi√®res + 6 milieu + 6 fin
    sampled_text = sample_document_pages(doc)
    
    # 3. Construire le prompt pour GPT-4o-mini
    prompt = build_identity_prompt(doc_title, toc, sampled_text)


    
    # 4. Appel API
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    
    try:
        response = await client.chat.completions.create(
            model="gpt-4.1-nano-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.02,  # Basse temp√©rature pour coh√©rence
            max_tokens=600,   # ~400 mots max
        )
        
        identity_text = response.choices[0].message.content.strip()
        token_count = response.usage.completion_tokens
        
        print(f"‚úÖ Fiche identit√© cr√©√©e : {token_count} tokens")
        
        return {
            "identity_text": identity_text,
            "token_count": token_count,
            "pages_sampled": sampled_text.get("pages_used", [])
        }
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation de la fiche identit√© : {e}")
        # Fallback : cr√©er une fiche minimale
        return create_fallback_identity(doc_title, toc)
    
def create_fallback_identity(doc_title: Optional[str], toc: str) -> Dict[str, Any]:
    # On force un nettoyage du sommaire pour s'assurer qu'il y a des retours √† la ligne
    formatted_toc = toc.replace(". ", ".\n- ") # Simple hack pour a√©rer si c'est coll√©

    identity_text = f"""
    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    üìã FICHE IDENTIT√â DU DOCUMENT
    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

    üìö TITRE: {doc_title or "Titre non d√©tect√©"}
    üìñ TYPE: Document religieux / √©ducatif
    üéØ SUJET: Contenu en cours d'analyse

    STRUCTURE DU DOCUMENT:
    - {formatted_toc}

    üîë TH√àMES CL√âS: √Ä d√©terminer
    üïå CONTEXTE: Islam / Acad√©mique

    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    """.strip()
    return {"identity_text": identity_text, "token_count": 0}

def extract_table_of_contents(doc: DoclingDocument) -> str:
    """
    Extrait le sommaire du document.
    
    L'API Docling utilise doc.iterate_items() ou doc.export_to_markdown()
    """
    toc_text = ""
    try:
        # Strat√©gie 1 : Utiliser export_to_markdown pour avoir la structure
        # (Docling g√©n√®re automatiquement les headings en Markdown)
        markdown = doc.export_to_markdown()
        
        # Extraire les lignes qui commencent par # (headings)
        lines = markdown.split('\n')
        headings = []
        
        for line in lines[:100]:  # Limiter aux 100 premi√®res lignes
            stripped = line.strip()
            if stripped.startswith('#'):
                # Nettoyer le heading (enlever les #)
                heading = stripped.lstrip('#').strip()
                
                # Filtrer les headings trop longs (probablement pas un titre)
                if heading and len(heading) < 100:
                    # D√©tecter si c'est un sommaire
                    if 'sommaire' in heading.lower() or 'table des mati√®res' in heading.lower():
                        # Extraire les 20 prochaines lignes apr√®s "Sommaire"
                        idx = lines.index(line)
                        toc_lines = lines[idx:idx+25]
                        toc_text = "\n".join([l.strip() for l in toc_lines if l.strip()])
                        return toc_text
                    
                    headings.append(heading)
        
        # Strat√©gie 2 : Si pas de sommaire explicite, retourner les headings trouv√©s
        if headings:
            toc_text = "\n".join(headings)
            return toc_text
        
        # Strat√©gie 3 : Fallback - It√©rer sur les items du document
        if not toc_text and hasattr(doc, 'body') and hasattr(doc.body, 'children'):
            for item in doc.body.children[:50]:
                if hasattr(item, 'label') and 'heading' in str(item.label).lower():
                    text = getattr(item, 'text', '').strip()
                    if text and len(text) < 100:
                        headings.append(text)
            
            if headings:
                toc_text = "\n".join(headings)

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur extraction sommaire : {e}")

    return toc_text or "Sommaire non d√©tect√©"

def sample_document_pages(doc: DoclingDocument, max_chars: int = 10000) -> Dict[str, Any]:
    try:
        full_markdown = doc.export_to_markdown()
        
        if len(full_markdown) <= max_chars:
            return {"text": full_markdown, "pages_used": ["Complet"]}

        # On d√©coupe par paragraphes (double saut de ligne) plut√¥t que par lignes
        # C'est plus s√©mantique pour le LLM
        paragraphs = full_markdown.split('\n\n')
        total_p = len(paragraphs)
        
        # √âchantillonnage : 15 d√©but, 15 milieu, 15 fin
        start_p = paragraphs[:15]
        mid_idx = total_p // 2
        mid_p = paragraphs[mid_idx-2 : mid_idx+7]
        end_p = paragraphs[-15:]
        
        sampled_text = (
            "--- D√âBUT DU DOCUMENT ---\n" + "\n\n".join(start_p) +
            "\n\n... [CONTENU INTERM√âDIAIRE] ...\n\n" + "\n\n".join(mid_p) +
            "\n\n... [CONTENU FINAL] ...\n\n" + "\n\n".join(end_p) +
            "\n--- FIN DU DOCUMENT ---"
        )

        return {
            "text": sampled_text[:max_chars], # S√©curit√© finale
            "pages_used": [0] #Symbolique, structure lin√©aire: 15 paragraphes de D√©but/Milieu/Fin du doc,chunk identit√© est sp√©cial
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur √©chantillonnage (fallback lignes) : {e}")
        return {"text": doc.export_to_markdown()[:max_chars], "pages_used": ["Fallback 10k chars"]}


def build_identity_prompt(
    doc_title: Optional[str], 
    toc: str, 
    sampled_text_data: Dict[str, Any]
) -> str:
    """
    Construit le prompt pour g√©n√©rer la fiche identit√©.
    """
    sampled_text = sampled_text_data.get("text", "")
    pages_used = sampled_text_data.get("pages_used", [])
    
    return f"""
Tu es un assistant sp√©cialis√© dans la cr√©ation de FICHES IDENTIT√â ultra-condens√©es pour des documents religieux et/ou √©ducatifs.

DOCUMENT ANALYS√â:
Titre: {doc_title or "Non sp√©cifi√©"}
Pages √©chantillonn√©es: {pages_used}

TABLE DES MATI√àRES:
{toc}

EXTRAITS DU DOCUMENT:
{sampled_text}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
T√ÇCHE: Cr√©e une FICHE IDENTIT√â ultra-condens√©e (MAX 400 mots).
TU DOIS IMP√âRATIVEMENT UTILISER DES RETOURS √Ä LA LIGNE ENTRE CHAQUE √âL√âMENT.
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

FORMAT STRICT √Ä RESPECTER :

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìã FICHE IDENTIT√â DU DOCUMENT
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìö TITRE: [titre exact]
üìñ TYPE: [biographie / cours / essai / etc.]
üéØ SUJET: [r√©sum√© en 2,3 phrases de quoi parle le document]

STRUCTURE DU DOCUMENT (SOMMAIRE) :
(Chaque chapitre DOIT √™tre sur une nouvelle ligne avec un tiret)
- 1. [Nom Chapitre] (p.[num√©ro])
- 2. [Nom Chapitre] (p.[num√©ro])
...

üîë TH√àMES CL√âS: [3-5 mots-cl√©s s√©par√©s par virgules]

üïå CONTEXTE: [√©poque, lieu, cadre si trouv√© dans les pages √©chantillonn√©es - 1,2 lignes max]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

R√àGLES D'OR DE MISE EN PAGE :
1. INTERDICTION FORMELLE de faire des paragraphes de texte compacts pour le sommaire. 
2. UN CHAPITRE = UNE LIGNE. C'est crucial pour la distinction s√©mantique.
3. Ne m√©lange jamais les noms de personnes ou de sections sur la m√™me ligne.
4. Les num√©ros de page sont ESSENTIELS.
5. Format ultra-scannable pour un LLM et un Reranker.

COMMENCE DIRECTEMENT PAR "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ..." (pas de pr√©ambule).
""".strip()
    
    return {
        "identity_text": identity_text,
        "token_count": len(identity_text.split()) * 1.3,  # Approximation
        "pages_sampled": []
    }


# Helper function (d√©j√† d√©finie dans separate_content_types.py mais r√©p√©t√©e ici pour clart√©)
def get_item_page(item) -> Optional[int]:
    """R√©cup√®re le num√©ro de page d'un item."""
    if hasattr(item, 'prov') and item.prov:
        for prov in item.prov:
            if hasattr(prov, 'page_no'):
                return prov.page_no
    return None

from docling_core.types.doc import DoclingDocument
import os
import json
from typing import Dict, Any, Optional
from openai import AsyncOpenAI


async def create_identity_chunk(
    doc: DoclingDocument, 
    doc_id: str,
    doc_title: Optional[str] = None
) -> Dict[str, Any]:
    """
    Cr√©e un chunk identit√© condens√© pour un document.
    
    Args:
        doc: Document Docling partitionn√©
        doc_id: UUID du document
        doc_title: Titre du document (optionnel)
    
    Returns:
        dict avec:
        - identity_text: Le texte de la fiche identit√©
        - token_count: Nombre approximatif de tokens
        - pages_sampled: Pages utilis√©es pour l'analyse
    """
    print(f"üîÑ Cr√©ation de la fiche identit√© pour {doc_title or doc_id}...")
    
    # 1. Extraire le sommaire/table des mati√®res
    toc_data = extract_table_of_contents(doc) #table of contents
    
    # 2. √âchantillonner le document : 6 premi√®res + 6 milieu + 6 fin
    sampled_text = sample_document_pages(doc)
    
    # 3. Construire le prompt pour GPT-4o-mini
    prompt = build_identity_prompt(doc_title, toc_data, sampled_text)


    
    # 4. Appel API
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    
    try:
        response = await client.chat.completions.create(
            model="gpt-4.1-nano-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.02,  # Basse temp√©rature pour coh√©rence
            max_tokens=600,   # ~400 mots max
        )
        
        identity_text = response.choices[0].message.content.strip()
        token_count = response.usage.completion_tokens
        
        print(f"‚úÖ Fiche identit√© cr√©√©e : {token_count} tokens")
        
        return {
            "identity_text": identity_text,
            "token_count": token_count,
            "pages_sampled": sampled_text.get("pages_used", [])
        }
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation de la fiche identit√© : {e}")
        # Fallback : cr√©er une fiche minimale
        return create_fallback_identity(doc_title, toc_data["content"])
    
def create_fallback_identity(doc_title: Optional[str], toc: str) -> Dict[str, Any]:
    # On force un nettoyage du sommaire pour s'assurer qu'il y a des retours √† la ligne
    formatted_toc = toc.replace(". ", ".\n- ") # Simple hack pour a√©rer si c'est coll√©

    identity_text = f"""
    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    üìã FICHE IDENTIT√â DU DOCUMENT
    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

    üìö TITRE: {doc_title or "Titre non d√©tect√©"}
    üìñ TYPE: Document religieux / √©ducatif
    üéØ SUJET: Contenu en cours d'analyse

    STRUCTURE DU DOCUMENT:
    - {formatted_toc}

    üîë TH√àMES CL√âS: √Ä d√©terminer
    üïå CONTEXTE: Islam / Acad√©mique

    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    """.strip()
    return {"identity_text": identity_text, "token_count": 0}

def extract_table_of_contents(doc: DoclingDocument) -> str:
    """
    Extrait le sommaire du document, qu'il soit au d√©but ou √† la fin.
    """
    try:
        markdown = doc.export_to_markdown()
        lines = markdown.split('\n')
        total_lines = len(lines)
        
        # 1. D√©finir les zones de recherche (D√©but et Fin)
        # On prend les 200 premi√®res et 200 derni√®res lignes
        search_limit = 200
        start_chunk = lines[:search_limit]
        end_chunk = lines[-search_limit:] if total_lines > search_limit else []
        
        # On combine pour la recherche de mots-cl√©s
        search_zones = [
            ("D√âBUT", start_chunk),
            ("FIN", end_chunk)
        ]
        
        keywords = ['sommaire', 'table des mati√®res', 'table of contents', 'plan du document']

        for zone_name, zone_lines in search_zones:
            for i, line in enumerate(zone_lines):
                stripped = line.strip()
                if any(kw in stripped.lower() for kw in keywords):
                    start_idx = i
                    end_idx = i + 50
                    toc_lines = zone_lines[start_idx:end_idx]
                    
                    print(f"üìç Sommaire d√©tect√© dans la zone : {zone_name}")
                    
                    # --- MODIFICATION ICI : RETOUR STRUCTURED ---
                    return {
                        "type": "OFFICIEL",
                        "content": "\n".join([l.strip() for l in toc_lines if l.strip()]),
                        "label": "TABLE DES MATI√àRES"
                    }

        # 2. Fallback : Si aucun mot-cl√© n'est trouv√©, on r√©cup√®re tous les titres (Headings)
        # Mais on se limite √† un nombre raisonnable pour la fiche d'identit√©
        headings = []
        for item in doc.iterate_items():
            # Docling marque les titres avec le label 'heading'
            if item.label == 'heading':
                text = item.text.strip()
                if text and len(text) < 120:
                    headings.append(text)
            
            if len(headings) > 60: # S√©curit√© pour ne pas saturer le prompt
                break
                
        if headings:
            return {
                "type": "ESTIM√â", 
                "content": "\n".join(headings), 
                "label": "STRUCTURE D√âTECT√âE (Titres principaux)"
            }

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur extraction sommaire : {e}")

    return {"type": "ABSENT", "content": "Non d√©tect√©", "label": "STRUCTURE INCONNUE"}

def sample_document_pages(doc: DoclingDocument, max_chars: int = 10000) -> Dict[str, Any]:
    try:
        full_markdown = doc.export_to_markdown()
        
        if len(full_markdown) <= max_chars:
            return {"text": full_markdown, "pages_used": ["Complet"]}

        # On d√©coupe par paragraphes (double saut de ligne) plut√¥t que par lignes
        # C'est plus s√©mantique pour le LLM
        paragraphs = full_markdown.split('\n\n')
        total_p = len(paragraphs)
        
        # √âchantillonnage : 15 d√©but, 15 milieu, 15 fin
        start_p = paragraphs[:15]
        mid_idx = total_p // 2
        mid_p = paragraphs[mid_idx-2 : mid_idx+7]
        end_p = paragraphs[-15:]
        
        sampled_text = (
            "--- D√âBUT DU DOCUMENT ---\n" + "\n\n".join(start_p) +
            "\n\n... [CONTENU INTERM√âDIAIRE] ...\n\n" + "\n\n".join(mid_p) +
            "\n\n... [CONTENU FINAL] ...\n\n" + "\n\n".join(end_p) +
            "\n--- FIN DU DOCUMENT ---"
        )

        return {
            "text": sampled_text[:max_chars], # S√©curit√© finale
            "pages_used": [0] #Symbolique, structure lin√©aire: 15 paragraphes de D√©but/Milieu/Fin du doc,chunk identit√© est sp√©cial
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur √©chantillonnage (fallback lignes) : {e}")
        return {"text": doc.export_to_markdown()[:max_chars], "pages_used": ["Fallback 10k chars"]}


def build_identity_prompt(
    doc_title: Optional[str], 
    toc_data: str, 
    sampled_text_data: Dict[str, Any]
) -> str:
    """
    Construit le prompt pour g√©n√©rer la fiche identit√©.
    """
    sampled_text = sampled_text_data.get("text", "")
    pages_used = sampled_text_data.get("pages_used", [])

    toc_type = toc_data.get('type')
    toc_content = toc_data.get('content')

    context_instruction = ""
    if toc_type == "OFFICIEL":
        context_instruction = "Utilise le sommaire officiel suivant pour comprendre l'organisation exacte du document."
    else:
        context_instruction = "Attention : Aucun sommaire officiel n'a √©t√© trouv√©. Voici une liste de titres extraits du corps du texte pour te donner une id√©e de la structure."
    
    return f"""
Tu es un assistant sp√©cialis√© dans la cr√©ation de FICHES IDENTIT√â ultra-condens√©es pour des documents religieux et/ou √©ducatifs.

DOCUMENT ANALYS√â:
Titre: {doc_title or "Non sp√©cifi√©"}
Pages √©chantillonn√©es: {pages_used}

STRUCTURE FOURNIE ({toc_data.get('label')}) :
    {toc_content}

EXTRAITS DU DOCUMENT:
{sampled_text}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
T√ÇCHE: Cr√©e une FICHE IDENTIT√â ultra-condens√©e (MAX 400 mots).
TU DOIS IMP√âRATIVEMENT UTILISER DES RETOURS √Ä LA LIGNE ENTRE CHAQUE √âL√âMENT.
{context_instruction}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

FORMAT STRICT √Ä RESPECTER :

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìã FICHE IDENTIT√â DU DOCUMENT
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìö TITRE: [titre exact]
üìñ TYPE: [biographie / cours / essai / etc.]
üéØ SUJET: [r√©sum√© en 2,3 phrases de quoi parle le document]

STRUCTURE DU DOCUMENT (SOMMAIRE/LISTE DES TITRES) :
(Chaque chapitre/titre DOIT √™tre sur une nouvelle ligne avec un tiret)
- 1. [Nom Chapitre] (p.[num√©ro])
- 2. [Nom Chapitre] (p.[num√©ro])
...

üîë TH√àMES CL√âS: [3-5 mots-cl√©s s√©par√©s par virgules]

üïå CONTEXTE: [√©poque, lieu, cadre si trouv√© dans les pages √©chantillonn√©es - 1,2 lignes max]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

R√àGLES D'OR DE MISE EN PAGE :
1. INTERDICTION FORMELLE de faire des paragraphes de texte compacts pour le sommaire/. 
2. UN CHAPITRE = UNE LIGNE. C'est crucial pour la distinction s√©mantique.
3. Ne m√©lange jamais les noms de personnes ou de sections sur la m√™me ligne.
4. Les num√©ros de page sont ESSENTIELS.
5. Format ultra-scannable pour un LLM et un Reranker.

COMMENCE DIRECTEMENT PAR "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ..." (pas de pr√©ambule).
""".strip()
    


# Helper function (d√©j√† d√©finie dans separate_content_types.py mais r√©p√©t√©e ici pour clart√©)
def get_item_page(item) -> Optional[int]:
    """R√©cup√®re le num√©ro de page d'un item."""
    if hasattr(item, 'prov') and item.prov:
        for prov in item.prov:
            if hasattr(prov, 'page_no'):
                return prov.page_no
    return None
